agent:
  class: agent.agent.SACAgent
  name: sac
  params:
    action_dim: ???
    action_range: ???
    actor_betas:
    - 0.9
    - 0.999
    actor_cfg: ${diag_gaussian_actor}
    actor_lr: ${sac_actor_lr}
    actor_update_frequency: 1
    alpha_betas:
    - 0.9
    - 0.999
    alpha_lr: ${sac_alpha_lr}
    batch_size: 1024
    critic_betas:
    - 0.9
    - 0.999
    critic_cfg: ${double_q_critic}
    critic_lr: ${sac_critic_lr}
    critic_target_update_frequency: 2
    critic_tau: 0.005
    device: ${device}
    discount: 0.99
    init_temperature: 0.1
    learnable_temperature: true
    obs_dim: ???
    robust_coef: 0.0005
    robust_method: l2_adv_param
device: cuda:0
diag_gaussian_actor:
  class: agent.actor.DiagGaussianActor
  params:
    action_dim: ${agent.params.action_dim}
    hidden_depth: 2
    hidden_dim: 1024
    log_std_bounds:
    - -5
    - 2
    obs_dim: ${agent.params.obs_dim}
double_q_critic:
  class: agent.critic.DoubleQCritic
  params:
    action_dim: ${agent.params.action_dim}
    hidden_depth: 2
    hidden_dim: 1024
    obs_dim: ${agent.params.obs_dim}
env:
  class: envs.rwrl_env.RWRLEnv
  name: quadruped_walk
  params:
    domain_name: quadruped
    task_kwargs:
      perturb_spec:
        enable: false
        period: 1
        scheduler: uniform
      random: ${seed}
    task_name: realworld_walk
eval_frequency: 10000.0
experiment: adv
log_frequency: 10000
log_save_tb: true
max_episode_steps: 1000
num_eval_episodes: 10
num_random_steps: 5000.0
num_train_steps: 1000000.0
replay_buffer_capacity: ${num_train_steps}
root_dir: ./outputs
sac_actor_lr: 0.0001
sac_alpha_lr: 0.0001
sac_critic_lr: 0.0001
save_video: false
seed: 12345
